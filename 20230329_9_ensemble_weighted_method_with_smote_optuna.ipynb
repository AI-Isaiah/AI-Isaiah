{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Isaiah/AI-Isaiah/blob/main/20230329_9_ensemble_weighted_method_with_smote_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuiTAYIhkrav",
        "outputId": "d324ac1e-4ab9-4473-965a-87d32fc580e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.22.4)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.47)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.3 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.1.1-cp39-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (23.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.15.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.1.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.svm import SVC\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "#!pip install verstack\n",
        "#from verstack import LGBMTuner\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from lightgbm.callback import early_stopping, log_evaluation\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sys\n",
        "import lightgbm as lgbm\n",
        "import logging\n",
        "sys.path.append('/bin/sample_data')\n",
        "sys.path.append('/root')\n",
        "#from preprocessor import generate_cm\n",
        "from collections import Counter\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTEENN\n",
        "import optuna\n",
        "import pickle\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, ParameterSampler, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import confusion_matrix, precision_score, log_loss, f1_score, roc_auc_score, roc_curve, recall_score, accuracy_score, make_scorer,ConfusionMatrixDisplay,matthews_corrcoef, classification_report\n",
        "from sklearn.feature_selection import RFECV\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import json\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gfRrZqwZTCU0"
      },
      "outputs": [],
      "source": [
        "def generate_cm1(y_true, y_pred, title):\n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Display confusion matrix using ConfusionMatrixDisplay\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "    disp.plot()\n",
        "    counter = Counter(y_true)\n",
        "    for key, value in counter.items():\n",
        "      print(f'{title}\"{key}\": {value}')\n",
        "\n",
        "class NumericalScaler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.num_cols = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "        self.scaler.fit(X[self.num_cols])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_scaled = X.copy()\n",
        "        X_scaled[self.num_cols] = self.scaler.transform(X[self.num_cols])\n",
        "        return X_scaled\n",
        "\n",
        "class FeatureEliminator:\n",
        "    def __init__(self, X_train, y_train, X_test):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "\n",
        "    @staticmethod\n",
        "    def objective(trial, X, y):\n",
        "        xgb_fe_params = {\n",
        "            \"tree_method\": \"hist\",\n",
        "            'objective': 'binary:logistic',\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 100, log=True),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 100, log=True),\n",
        "        }\n",
        "\n",
        "        dtrain = xgb.DMatrix(X, label=y, enable_categorical=True, eval_metric = \"logloss\")\n",
        "        num_boost_round = trial.suggest_int('num_boost_round', 50, 1000)\n",
        "        #model = xgb.train(params, dtrain, num_boost_round = num_boost_round)\n",
        "        #y_pred = model.predict(dtrain)\n",
        "        cv_result = xgb.cv(xgb_fe_params, \n",
        "                       dtrain, nfold=5, \n",
        "                       num_boost_round = num_boost_round, \n",
        "                       stratified=True, \n",
        "                       early_stopping_rounds=50, \n",
        "                       seed = 42, \n",
        "                       maximize = False)\n",
        "    \n",
        "        min_logloss_val = cv_result[\"test-logloss-mean\"].min()\n",
        "    \n",
        "        return min_logloss_val\n",
        "        \n",
        "    def eliminate_features(self, correlated_features):\n",
        "        # Remove the least important feature from correlated pairs\n",
        "        for feature_pair in correlated_features:\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            study.optimize(lambda trial: self.objective(trial, self.X_train[[feature_pair[0]]].values, self.y_train), n_trials=50)\n",
        "            score_0 = study.best_value\n",
        "\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "            study.optimize(lambda trial: self.objective(trial, self.X_train[[feature_pair[1]]].values, self.y_train), n_trials=50)\n",
        "            score_1 = study.best_value\n",
        "\n",
        "            if score_0 < score_1:\n",
        "                self.X_train.drop(feature_pair[1], axis=1, inplace=True)\n",
        "            else:\n",
        "                self.X_train.drop(feature_pair[0], axis=1, inplace=True)\n",
        "\n",
        "        # Update X_train and X_test after removing correlated features\n",
        "        self.X_test = pd.DataFrame(self.X_test, columns=self.X_train.columns)\n",
        "\n",
        "def find_correlated_features(df, threshold=0.9):\n",
        "    correlated_features = set()\n",
        "    correlation_matrix = df.corr()\n",
        "\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "                colname_i = correlation_matrix.columns[i]\n",
        "                colname_j = correlation_matrix.columns[j]\n",
        "                correlated_features.add((colname_i, colname_j))\n",
        "\n",
        "    return list(correlated_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "VVbI_tqWkrax",
        "outputId": "6a571081-2df0-4501-8ce1-d183dff03deb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9143eeae4269>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/loan_book_data_labels.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/loan_book_data_labels.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/loan_book_data_labels.csv\")\n",
        "labels = data[\"Result\"]\n",
        "data.drop(\"Result\", axis=1, inplace=True)\n",
        "print(labels.head())\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOQGdMWTkray"
      },
      "outputs": [],
      "source": [
        "X = data\n",
        "if \"appl_ref\" in data.columns:\n",
        "    # drop the column\n",
        "    data.drop(\"appl_ref\", axis=1, inplace=True)\n",
        "\n",
        "for i in range(52, 70):\n",
        "  column_name = \"v\" + str(i) + \"_src\"\n",
        "  if column_name in data.columns:\n",
        "    data.drop(column_name, axis=1, inplace=True)\n",
        "\n",
        "X = data\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = X[col].astype('category')\n",
        "print(X.dtypes)\n",
        "feature_names = X.columns\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "y = y.ravel()\n",
        "cat_columns = X.select_dtypes(include='category').columns\n",
        "\n",
        "num_columns = X.select_dtypes(exclude=['category']).columns\n",
        "\n",
        "# Encode categorical columns using LabelEncoder\n",
        "\"\"\"label_encoders = {}\n",
        "for col in cat_columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxn2MRMfkraz"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "y_train = pd.DataFrame(y_train, columns=[\"Result\"])\n",
        "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
        "y_test = pd.DataFrame(y_test, columns=[\"Result\"])\n",
        "\n",
        "#Get the correlated foreatures from X_train\n",
        "correlated_features = find_correlated_features(X_train, threshold=0.8)\n",
        "\n",
        "#instantiate class FeatureEliminator\n",
        "fe = FeatureEliminator(X_train, y_train, X_test)\n",
        "\n",
        "# Call the eliminate_features method with the correlated_features list\n",
        "fe.eliminate_features(correlated_features)\n",
        "\n",
        "# Get the updated X_train and X_test\n",
        "X_train = fe.X_train\n",
        "X_test = fe.X_test\n",
        "\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = X_train.select_dtypes(include=['category']).columns.tolist()\n",
        "print(\"Cat-cols: \", cat_cols)\n",
        "num_cols = X_train.select_dtypes(exclude=['category']).columns.tolist()\n",
        "# Create a SMOTENC object with categorical features\n",
        "smote_nc = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in cat_cols], random_state=42)\n",
        "\n",
        "# Fit the SMOTENC object to the training data\n",
        "X_train, y_train = smote_nc.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"X_train_shape final: \", X_train.shape)\n",
        "print(\"X_train dtypes:\" , X_train.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83sm-6FIkra0"
      },
      "outputs": [],
      "source": [
        "preprocessor = NumericalScaler()\n",
        "\n",
        "# Fit the preprocessor on the training data\n",
        "X_train= preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Apply the preprocessor on the validation and test data\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhab7Vp5QMBW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXFnU8p0EH1R"
      },
      "source": [
        "# Random Forest Classifier \n",
        "\n",
        "\n",
        "1.   Optuna - XXX algorithm \n",
        "2.   cross_val with StratKFold\n",
        "3. Scoring Neg Log loss\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEeIXGt0kra1"
      },
      "outputs": [],
      "source": [
        " #Define the objective function for Optuna\n",
        "def objective(trial, X, y):\n",
        "    \"n_estimators\" : trial.suggest_int(\"n_estimators\", 10, 2000)\n",
        "    \"criterion\" : trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"mse\", \"mae\"])\n",
        "    \"max_depth\" : trial.suggest_int(\"max_depth\", 2, 32, log=True)\n",
        "    \"min_samples_split\" : trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    \"min_samples_leaf\" : trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
        "    \"max_features\" : trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n",
        "    \"bootstrap\" : True,\n",
        "    \"class_weight\" : trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"])\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        criterion=criterion,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        bootstrap=bootstrap,\n",
        "        class_weight=class_weight,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_log_loss\")\n",
        "    mean_score = np.mean(scores)\n",
        "\n",
        "    return mean_score\n",
        "\n",
        "# Optimize hyperparameters with Optuna\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.NSGAIISampler(seed=42))\n",
        "study.optimize(lambda trial: objective(trial, X, y), n_trials=100, timeout=600)\n",
        "\n",
        "# Print best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"Value: \", trial.value)\n",
        "print(\"Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3qpMg6E0XE"
      },
      "source": [
        "# ElasticNetCV \n",
        "\n",
        "\n",
        "1.   Simple native algorithm\n",
        "2. One Hot encoding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYSwQ_z6kra2"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding of selected categorical features\n",
        "X_train_ohe = pd.get_dummies(X_train, columns=cat_cols)\n",
        "X_val_ohe = pd.get_dummies(X_val, columns=cat_cols)\n",
        "X_val_ohe = pd.get_dummies(X_test, columns=cat_cols)\n",
        "\n",
        "\n",
        "ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
        "                                n_alphas=100,\n",
        "                                cv=10,\n",
        "                                max_iter=10000,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1))\n",
        "])\n",
        "# ElasticNetCV model\n",
        "l1_ratios = np.linspace(.01,.05, .1, .2, .3, .4, .5, .6, .7, .9, .95, .99, 1)\n",
        "model = ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=10, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train_ohe, y_train_ohe)\n",
        "\n",
        "# Prediction and evaluation\n",
        "y_pred = model.predict(X_val_ohe)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "print(f\"Optimal l1_ratio: {model.l1_ratio_}\")\n",
        "print(f\"Optimal alpha: {model.alpha_}\")\n",
        "print(f\"Mean squared error: {mse}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3CwqnFhEyzz"
      },
      "source": [
        "# LGBM Classifier \n",
        "\n",
        "\n",
        "1.   Optuna - XXX algorithm \n",
        "2.   cross_val with StratKFold\n",
        "3. Scoring Neg Log loss\n",
        "4. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP3kkZe3kra2"
      },
      "outputs": [],
      "source": [
        "def objective_lgbm(trial, X, y):\n",
        "    \n",
        "    lgbm_params = {\n",
        "      'objective': 'binary',\n",
        "      'boosting_type': 'gbdt',\n",
        "      \"metric\": \"binary_logloss\",\n",
        "      'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "      'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3),\n",
        "      'num_leaves': trial.suggest_int('num_leaves', 20, 80),\n",
        "      'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
        "      'min_child_weight': trial.suggest_float('min_child_weight', 1e-5, 100),\n",
        "      'subsample': trial.suggest_float('subsample', 0.1, 1.0, step=0.1),\n",
        "      'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
        "      'reg_alpha': trial.suggest_float('reg_alpha', 0, 10, step=0.1),\n",
        "      'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
        "      'min_split_gain': trial.suggest_float('min_split_gain', 0, 1),\n",
        "      'max_bin': trial.suggest_int('max_bin', 300, 800),\n",
        "      'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "      'verbosity': -1,\n",
        "      'seed': 42\n",
        "    }\n",
        "\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    num_boost_round = trial.suggest_int('num_boost_round', 50, 1000)\n",
        "    cv_scores = np.empty(5)\n",
        "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        lgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)\n",
        "        lgbm_valid = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)#, reference=lgbm_train)\n",
        "        \n",
        "        #model = lgbm.LGBMClassifier(**lgbm_params)\n",
        "        model = lgbm.train(lgbm_params, \n",
        "                           lgbm_train,\n",
        "                           num_boost_round = num_boost_round,\n",
        "                           valid_sets=[lgbm_valid],\n",
        "                           callbacks=[early_stopping(stopping_rounds=100, first_metric_only=False),\n",
        "                                      log_evaluation(period=10)],\n",
        "        )\n",
        "        preds = model.predict(X_test)\n",
        "        cv_scores[idx] = log_loss(y_test, preds)\n",
        "    print(f'Trial {trial.number}: Parameters: {lgbm_params}, Logloss: {np.mean(cv_scores)}')\n",
        "    return -np.mean(cv_scores)\n",
        "    \n",
        "starting_lgbm_hp = {\n",
        "  \n",
        "}\n",
        "#optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
        "#optuna.logging.set_verbosity(10)\n",
        "study = optuna.create_study(direction='maximize', \n",
        "                            sampler=optuna.samplers.NSGAIISampler(seed=42), \n",
        "                            pruner=optuna.pruners.HyperbandPruner(),\n",
        "                            )\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "study_logger = logging.getLogger(\"optuna\")\n",
        "study_logger.setLevel(logging.WARNING)\n",
        "study_logger.addHandler(logging.StreamHandler())\n",
        "\n",
        "study.enqueue_trial(starting_lgbm_hp)\n",
        "study.optimize(lambda trial: objective_lgbm(trial, X_train, y_train), \n",
        "               n_trials=18000, \n",
        "               show_progress_bar=True)\n",
        "\n",
        "best_params = study.best_params\n",
        "print(f\"Best log loss: {study.best_value:.4f}\")\n",
        "print(f\"Best params: {best_params}\")\n",
        "\n",
        "lgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)\n",
        "lgbm_val = lgbm.Dataset(X_val, y_val, categorical_feature=cat_cols)#, reference=lgbm_train)\n",
        "lgbm_test = lgbm.Dataset(X_test, categorical_feature=cat_cols)#, reference=lgbm_val)\n",
        "\n",
        "lgbm_clf = lgbm.train(best_params, lgbm_train)\n",
        "y_pred = lgbm_clf.predict(X_test)\n",
        "\n",
        "y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred]\n",
        "print(\"y_pred_binary_len: \", len(y_pred_binary))\n",
        "print(\"y_test shape: \", y_test.shape)\n",
        "accuracy = sum(y_pred_binary == y_test.squeeze()) / len(y_test)\n",
        "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "lgbm_best_trial = study.best_trial\n",
        "lgbm_best_params = lgbm_best_trial.params\n",
        "\n",
        "print(f\"Best score: {lgbm_best_trial.value:.4f}\")\n",
        "print(f\"Best params: {lgbm_best_params}\")\n",
        "\n",
        "with open('trial_lgbm.pickle', 'wb') as f:\n",
        "    pickle.dump(lgbm_best_params, f)\n",
        "\n",
        "\n",
        "generate_cm1(y_val, lgbm_clf.predict(X_val), \"XGBoost, optuna - cv, Validation Set\")\n",
        "generate_cm1(y_test, lgbm_clf.predict(X_test), \"XGBoost, optuna - cv, Validation Set\")\n",
        "\n",
        "\n",
        "print(\"Counter - valid\" , y_val.value_counts())\n",
        "print(\"Counter - test\", y_test.value_counts())\n",
        "\n",
        "\n",
        "with open('lgbm_best_params.json', 'w') as f:\n",
        "    json.dump(lgbm_best_params, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPZvbLwpT8Yf"
      },
      "outputs": [],
      "source": [
        "def custom_loss_xgb(y_pred, dtrain):\n",
        "    y_true = dtrain.get_label()\n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    custom_metric = (tn * 4) - (fn * 10)\n",
        "    return 'custom_loss', custom_metric\n",
        "\n",
        "def custom_obj(y_true, y_pred):\n",
        "    grad = (y_pred - y_true)\n",
        "    hess = np.ones_like(y_true)\n",
        "    fn_mask = y_true == 0  # identify false negatives\n",
        "    grad[fn_mask] *= 3  # triple-penalize false negatives\n",
        "    hess[fn_mask] *= 2  # increase penalty for false negatives\n",
        "    tn_mask = y_true == 1  # identify true negatives\n",
        "    grad[tn_mask] += 2  # give bonus for true negatives\n",
        "    hess[tn_mask] *= 2  # increase bonus for true negatives\n",
        "    return grad, hess\n",
        "\n",
        "class CustomMetric:\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        custom_metric = (tn * 4) - (fn * 10)\n",
        "        return 'custom_metric', custom_metric\n",
        "custom_metric = CustomMetric()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEn-9YwUkra3"
      },
      "outputs": [],
      "source": [
        "def custom_obj(y_true, y_pred):\n",
        "    y_pred = y_pred.get_label()\n",
        "    grad = y_pred - y_true\n",
        "    hess = np.ones_like(y_true)\n",
        "\n",
        "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "    fn_mask = (y_true == 1) & (y_pred_labels == 0)  # identify false negatives\n",
        "    grad[fn_mask] -= 3  # triple-penalize false negatives\n",
        "    hess[fn_mask] *= 2  # increase penalty for false negatives\n",
        "\n",
        "    tn_mask = (y_true == 0) & (y_pred_labels == 0)  # identify true negatives\n",
        "    grad[tn_mask] += 2  # give bonus for true negatives\n",
        "    hess[tn_mask] *= 2  # increase bonus for true negatives\n",
        "\n",
        "    return grad, hess\n",
        "class CustomMetric:\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        y_pred = y_pred.get_label()\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "        y_true = (y_true > 0.5).astype(int)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        custom_metric = (tn * 4) - (fp * 4)\n",
        "        return 'custom_metric', custom_metric\n",
        "custom_metric = CustomMetric()\n",
        "\n",
        "class CustomMetric2:\n",
        "  def custom_eval(preds, dmatrix):\n",
        "      labels = dmatrix.get_label()\n",
        "      preds_binary = (preds > 0.5).astype(int)\n",
        "      mcc = matthews_corrcoef(labels, preds_binary)\n",
        "      fn_mask = labels == 1  # identify false negatives\n",
        "      tn_mask = labels == 0  # identify true negatives\n",
        "      mcc_fn = mcc * (1 - sum(fn_mask) / len(labels))  # multiply MCC by the fraction of true negatives\n",
        "      mcc_tn = mcc * sum(tn_mask) / len(labels)  # multiply MCC by the fraction of false negatives\n",
        "      return \"mcc\", (mcc_fn + mcc_tn) / 2, True\n",
        "custom_metric2 = CustomMetric2()\n",
        "\n",
        "def objective_xgb(trial,study, X, y):\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "    #dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    #dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "    #dtrain = xgb.DMatrix(X, label=y)\n",
        "    xgb_params = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.1, 0.6),\n",
        "        \"tree_method\": \"hist\",\n",
        "        'objective': 'binary:logistic',\n",
        "        \"eval_metric\": \"logloss\",\n",
        "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 0.4),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 0.6),\n",
        "        \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-8, 0.6),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 0.4),\n",
        "        \"verbosity\": 0\n",
        "    }\n",
        "\n",
        "    if xgb_params[\"booster\"] == \"gbtree\" or xgb_params[\"booster\"] == \"dart\":\n",
        "        xgb_params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 16)\n",
        "        xgb_params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 0.6)\n",
        "        xgb_params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 0.6)\n",
        "        xgb_params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "    if xgb_params[\"booster\"] == \"dart\":\n",
        "        xgb_params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "        xgb_params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "        xgb_params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 0.6)\n",
        "        xgb_params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 0.6)\n",
        "\n",
        "    #pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
        "    #pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'logloss')\n",
        "    #bst = xgb.train(xgb_params, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n",
        "    dtrain = xgb.DMatrix(X, label=y, enable_categorical = True)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val, enable_categorical = True)\n",
        "    evals = [(dtrain, 'train'), (dval, 'val')]\n",
        "    num_boost_round = trial.suggest_int('num_boost_round', 50, 1000)\n",
        "    cv_result = xgb.cv(xgb_params, \n",
        "                       dtrain, nfold=5, \n",
        "                       num_boost_round = num_boost_round, \n",
        "                       stratified=True, \n",
        "                       early_stopping_rounds=50, \n",
        "                       seed = 42, \n",
        "                       maximize = False)\n",
        "    \n",
        "    logloss_train = cv_result[\"train-logloss-mean\"].iloc[-1]\n",
        "    logloss_val = cv_result[\"test-logloss-mean\"].iloc[-1]\n",
        "\n",
        "    min_logloss_val = cv_result[\"test-logloss-mean\"].min()\n",
        "    \n",
        "    return -min_logloss_val\n",
        "   \n",
        "    #logloss = cv_result[\"test-logloss-mean\"].min()\n",
        "    #return logloss\n",
        "    \"\"\"\n",
        "    mean_auc = cv_result['test-auc-mean'].max()\n",
        "    #mean_error = cv_result[\"test-error-mean\"].min()\n",
        "    #return 1.0 - mean_error\n",
        "    #y_preds = bst.predict(dtest)\n",
        "    #y_preds_labels = np.rint(y_preds)\n",
        "    #accuracy = accuracy_score(y_test, y_preds_labels)\n",
        "    #return accuracy\n",
        "    #return mean_auc\n",
        "    n_splits = 5\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "    num_boost_round = trial.suggest_int('num_boost_round', 50, 500)\n",
        "    # Perform cross-validation with the custom objective and evaluation metric\n",
        "    for train_index, val_index in skf.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "        dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
        "        dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
        "\n",
        "        # Create a dictionary to store the evaluation results\n",
        "        evals_result = {}\n",
        "\n",
        "        # Train the model using the custom objective and evaluation metric\n",
        "        bst = xgb.train(\n",
        "            xgb_params,\n",
        "            dtrain,\n",
        "            num_boost_round=num_boost_round,\n",
        "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "            obj=\"binary:logistic\"\n",
        "            custom_metric=\"accuracy\"\n",
        "            evals_result=evals_result,\n",
        "            early_stopping_rounds=10,\n",
        "            maximize=True,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "\n",
        "        # Get the best validation score\n",
        "        best_val_score = max(evals_result['val']['custom_metric'])\n",
        "        scores.append(best_val_score)\n",
        "\n",
        "    # Calculate the mean validation score\n",
        "    mean_val_score = np.mean(scores)\n",
        "\n",
        "    return mean_val_score\"\"\"\n",
        "\n",
        "starting_xgb_hp = {\n",
        "    'learning_rate': 0.07937792496092369, \n",
        "    'min_child_weight': 1, \n",
        "    'subsample': 0.5072873601156911, \n",
        "    'booster': 'gbtree', \n",
        "    'lambda': 0.2101881276876542, \n",
        "    'alpha': 0.1022775152443601, \n",
        "    'reg_lambda': 0.15380633163734847, \n",
        "    'max_depth': 6, \n",
        "    'eta': 0.25908426274659535, \n",
        "    'gamma': 0.46906789773943747, \n",
        "    'grow_policy': 'lossguide', \n",
        "    'num_boost_round': 368\n",
        "}\n",
        "#optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
        "#optuna.logging.set_verbosity(10)\n",
        "study = optuna.create_study(direction='maximize', \n",
        "                            sampler=optuna.samplers.NSGAIISampler(seed=42), \n",
        "                            pruner=optuna.pruners.HyperbandPruner(min_resource=1, \n",
        "                                                                  max_resource=\"auto\", \n",
        "                                                                  reduction_factor=3),\n",
        "                            )\n",
        "study_logger = logging.getLogger(\"optuna\")\n",
        "study_logger.setLevel(1)\n",
        "study_logger.addHandler(logging.StreamHandler())\n",
        "\n",
        "study.enqueue_trial(starting_xgb_hp)\n",
        "study.optimize(lambda trial: objective_xgb(trial, study, X_train, y_train), \n",
        "               n_trials=600, \n",
        "               show_progress_bar=True)\n",
        "\n",
        "\n",
        "xgb_best_trial = study.best_trial\n",
        "xgb_best_params = xgb_best_trial.params\n",
        "\n",
        "print(f\"Best score: {xgb_best_trial.value:.4f}\")\n",
        "print(f\"Best params: {xgb_best_params}\")\n",
        "\n",
        "with open('trial_xgb.pickle', 'wb') as f:\n",
        "    pickle.dump(xgb_best_params, f)\n",
        "\n",
        "\n",
        "#xgb_clf = XGBClassifier(**xgb_best_params, tree_method=\"gpu_hist\", enable_categorical=True, max_to_one_hot=1)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical = True)\n",
        "dval = xgb.DMatrix(X_val, label=y_val, enable_categorical = True)\n",
        "evals = [(dtrain, 'train'), (dval, 'val')]\n",
        "xgb_clf = xgb.train(xgb_best_params, dtrain, num_boost_round=1000, early_stopping_rounds=20, evals=evals)\n",
        "\n",
        "dX_val =  xgb.DMatrix(X_val, enable_categorical = True)\n",
        "dX_test = xgb.DMatrix(X_test, enable_categorical = True)\n",
        "generate_cm1(y_val, xgb_clf.predict(dX_val), \"XGBoost, optuna - cv, Validation Set\")\n",
        "generate_cm1(y_test, xgb_clf.predict(dX_test), \"XGBoost, optuna - cv, Validation Set\")\n",
        "\n",
        "print(\"Counter - valid\" , y_val.value_counts())\n",
        "print(\"Counter - test\", y_test.value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ4vZozhHNdW"
      },
      "outputs": [],
      "source": [
        "y_pred = xgb_clf.predict(dX_val)\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "print(\"confusion Matrix validation\")\n",
        "print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catboost classifier"
      ],
      "metadata": {
        "id": "mNbjIOsk3FoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_pred, y_true):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return (tn * 4) - (fn * 10)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective_catboost(trial, X, y, cat_cols):\n",
        "    params = {\n",
        "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
        "        'depth': trial.suggest_int('depth', 2, 10),\n",
        "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
        "        'border_count': trial.suggest_int('border_count', 5, 200),\n",
        "        'random_strength': trial.suggest_int('random_strength', 1, 20),\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.01, 100.00),\n",
        "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
        "        'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 1, 10),\n",
        "        'task_type': 'GPU'\n",
        "    }\n",
        "    \n",
        "    n_splits = 5\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Train and evaluate the model using cross-validation\n",
        "    scores = []\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "        \n",
        "        # Create a Pool object with categorical features\n",
        "        pool_train = Pool(X_train, y_train, cat_features=cat_cols)\n",
        "        pool_valid = Pool(X_val, y_val, cat_features=cat_cols)\n",
        "        \n",
        "        # Train the model\n",
        "        model = CatBoostClassifier(**params, random_state = 42)\n",
        "        model.fit(pool_train, eval_set=pool_valid, verbose=False)\n",
        "        \n",
        "        # Evaluate the model on the validation set\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "        score = custom_loss(y_val, y_pred)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "#Starting hyperparamters to improve convergence\n",
        "starting_cat_hp = {'iterations': 109, \n",
        "                            'learning_rate': 0.45363716844543917, \n",
        "                            'depth': 2, \n",
        "                            'l2_leaf_reg': 4, \n",
        "                            'border_count': 191, \n",
        "                            'random_strength': 20, \n",
        "                            'bagging_temperature': 57.34805443344737, \n",
        "                            'grow_policy': 'SymmetricTree', \n",
        "                            'max_ctr_complexity': 4\n",
        "}\n",
        "\n",
        "# Optimize hyperparameters\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.NSGAIISampler(seed = 42))\n",
        "study.enqueue_trial(starting_cat_hp)\n",
        "study.optimize(lambda trial: objective_catboost(trial, X_train,y_train, cat_cols), n_trials=100)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = study.best_params\n",
        "print(\"Best parameters: \", best_params)\n",
        "\n",
        "#Pool variables to be used with catboost:\n",
        "pool_train = Pool(X_train, y_train, cat_features = cat_cols)\n",
        "pool_valid = Pool(X_val, cat_features = cat_cols)\n",
        "pool_test = Pool(X_test, cat_features = cat_cols)\n",
        "\n",
        "\n",
        "# Run the randomized search\n",
        "# best parameters: iterations': 420, 'learning_rate': 0.18951730321390894, 'depth': 2, 'l2_leaf_reg': 2, 'border_count': 44, 'random_strength': 11, 'bagging_temperature': 59.24553274051562, 'grow_policy': 'SymmetricTree', 'max_ctr_complexity': 5\n",
        "cb_classifier = CatBoostClassifier(**best_params, random_state=42, verbose=0)\n",
        "cb_classifier.fit(pool_train)\n",
        "tn, fp, fn, tp = confusion_matrix(y_val, cb_classifier.predict(X_val)).ravel()\n",
        "print(\"confusion Matrix validation\")\n",
        "print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp)\n",
        "generate_cm(y_val, cb_classifier.predict(pool_valid), \"CatBoost - Validation\")\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, cb_classifier.predict(X_test)).ravel()\n",
        "print(\"confusion Matrix test\")\n",
        "print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp)\n",
        "generate_cm(y_test, cb_classifier.predict(pool_test), \"CatBoost - Test\")\n",
        "\n",
        "# Save the best parameters for CatBoost\n",
        "with open('cat_best_params.json', 'w') as f:\n",
        "    json.dump(study.best_params, f)\n",
        "\n",
        "# Clear memory\n",
        "del cb_classifier\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "_6ZhGO733IPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB1fzj8r_KLH"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_eval_metric = evals_result['train']['custom_metric']\n",
        "test_eval_metric = evals_result['test']['custom_metric']\n",
        "\n",
        "# Visualize the evaluation metric\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_eval_metric, label='Train')\n",
        "plt.plot(test_eval_metric, label='Test')\n",
        "plt.xlabel('Boosting round')\n",
        "plt.ylabel('Custom metric')\n",
        "plt.title('Evaluation metric during training process')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frFZ_RdRkra3"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Load the best parameters for each classifier\n",
        "with open('lgbm_best_params.pkl', 'rb') as f:\n",
        "lgbm_best_params = pickle.load(f)\n",
        "\n",
        "with open(\"trial_xgb.pkl\", \"rb\") as f:\n",
        "xgb_trial = pickle.load(f)\n",
        "\n",
        "with open('cat_best_params.pkl', 'rb') as f:\n",
        "cat_best_params = pickle.load(f)\n",
        "\n",
        "with open('logreg_best_params.pkl', 'rb') as f:\n",
        "logreg_best_params = pickle.load(f)\n",
        "\n",
        "with open('rf_best_params.pkl', 'rb') as f:\n",
        "rf_best_params = pickle.load(f)\n",
        "\n",
        "with open('svm_best_params.pkl', 'rb') as f:\n",
        "svm_best_params = pickle.load(f)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jafTzVDLkra3"
      },
      "outputs": [],
      "source": [
        "# Instantiate classifiers with the best parameters\n",
        "lgbm_clf = LGBMClassifier(random_state=42, **lgbm_best_params)\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, random_state=42, tree_method='hist', n_jobs=-1, **xgb_best_params)\n",
        "cat_clf = CatBoostClassifier(task_type=\"CPU\", random_seed=42, verbose = False, **cat_best_params)\n",
        "rf_clf = RandomForestClassifier(random_state=42, **rf_best_params)\n",
        "ela_clf = \n",
        "\n",
        "# Create a soft-voting classifier\n",
        "ensemble_clf = VotingClassifier(estimators=[\n",
        "    ('lgbm', lgbm_clf),\n",
        "    ('xgb', xgb_clf),\n",
        "    ('cat', cat_clf),\n",
        "    ('logreg', logreg_clf),\n",
        "    ('rf', rf_clf),\n",
        "    ('svm', svm_clf)\n",
        "], voting='soft')\n",
        "\n",
        "\n",
        "\n",
        "# Train the ensemble classifier\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the recall score for the ensemble\n",
        "ensemble_recall = recall_score(y_true=y_test, y_pred=ensemble_clf.predict(X_test), pos_label=1)\n",
        "print(f\"Ensemble recall score: {ensemble_recall}\")\n",
        "\n",
        "generate_cm(y_val, ensemble_clf.predict(X_val))\n",
        "generate_cm(y_test, ensemble_clf.predict(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjmuYe8qkra3"
      },
      "outputs": [],
      "source": [
        "y_pred = ensemble_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSIzz7Wnkra4"
      },
      "outputs": [],
      "source": [
        "y_pred = ensemble_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDcehakvkra4"
      },
      "outputs": [],
      "source": [
        "precision = precision_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
        "recall = recall_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCpwFDOakra4"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = ensemble_clf.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_true=y_test, y_score=y_pred_proba)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true=y_test, y_score=y_pred_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"Ensemble Classifier (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkwv32W-kra4"
      },
      "source": [
        "Addition of feauture reduction methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG03O_2Gkra5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "def voting_classifier_feature_importances(voting_clf):\n",
        "    importances = []\n",
        "    for clf in voting_clf.estimators_:\n",
        "        if isinstance(clf, tuple):  # Check if the clf is a tuple (name, estimator)\n",
        "            clf = clf[1]\n",
        "        if hasattr(clf, 'coef_'):\n",
        "            importances.append(clf.coef_.flatten())\n",
        "        elif hasattr(clf, 'feature_importances_'):\n",
        "            importances.append(clf.feature_importances_)\n",
        "    \n",
        "    if not importances:\n",
        "        raise ValueError(\"No estimators in the ensemble have `coef_` or `feature_importances_` attribute.\")\n",
        "    \n",
        "    return np.mean(importances, axis=0)\n",
        "\n",
        "\n",
        "rfe = RFECV(estimator=ensemble_clf, step=1, cv=5, scoring='recall', n_jobs=-1, importance_getter=voting_classifier_feature_importances)\n",
        "\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    with redirect_stdout(devnull):\n",
        "        rfe.fit(X_train, y_train)\n",
        "\n",
        "# Print the optimal number of features\n",
        "print(\"Optimal number of features:\", rfe.n_features_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cevSlb2zkra5"
      },
      "outputs": [],
      "source": [
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "# Print the optimal number of features\n",
        "print(\"Optimal number of features:\", rfe.n_features_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJdTPStqkra5"
      },
      "outputs": [],
      "source": [
        "ensemble_clf.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = ensemble_clf.predict(X_test_rfe)\n",
        "\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "report_rfe = classification_report(y_test, y_pred_rfe)\n",
        "conf_matrix_rfe = confusion_matrix(y_test, y_pred_rfe)\n",
        "\n",
        "print(f\"Accuracy (with RFE): {accuracy_rfe:.4f}\")\n",
        "print(\"Classification Report (with RFE):\")\n",
        "print(report_rfe)\n",
        "print(\"Confusion Matrix (with RFE):\")\n",
        "print(conf_matrix_rfe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodr_9e9kra6"
      },
      "source": [
        "Use stacking method to combine the results of different models\n",
        "Extra trees classifier and Principal component analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQWHxfm9kra6"
      },
      "outputs": [],
      "source": [
        "ex_clf = ExtraTreesClassifier(random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pca', PCA()),\n",
        "    ('ex_clf', ex_clf),\n",
        "])\n",
        "param_grid = {\n",
        "    'pca__n_components': np.arange(1, X_train.shape[1] + 1),\n",
        "    'ex_clf__n_estimators': [100, 200, 500],\n",
        "    'ex_clf__max_depth': [None, 10, 20, 30],\n",
        "    'ex_clf__min_samples_split': [2, 5, 10],\n",
        "    'ex_clf__min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(pipeline, param_grid, n_iter=50, cv=5, scoring='recall', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "print(\"Best score:\", random_search.best_score_)\n",
        "ex_clf_best_params = random_search.best_params_\n",
        "\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6DTlRckra6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Mrf_qNkra6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(X_train.shape[1]), importances[sorted_indices])\n",
        "plt.xticks(range(X_train.shape[1]), sorted_indices)\n",
        "plt.xlabel(\"Feature index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Feature importances in the best ExtraTreesClassifier model\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOa55XBekra6"
      },
      "source": [
        "Use the other classifiers to predict the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6SAIsSVkra6"
      },
      "outputs": [],
      "source": [
        "# Instantiate classifiers\n",
        "\n",
        "lgbm_clf = LGBMClassifier(random_state=42, **lgbm_best_params)\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, random_state=42, tree_method='hist', n_jobs=-1, **xgb_best_params)\n",
        "cat_clf = CatBoostClassifier(task_type=\"CPU\", random_seed=42, **cat_best_params)\n",
        "rf_clf = RandomForestClassifier(random_state=42, **rf_best_params)\n",
        "ElastinNetCV\n",
        "\n",
        "# Create pipelines with PCA and classifiers\n",
        "pipelines = {\n",
        "    \n",
        "    'LightGBM': Pipeline([('pca', PCA()), ('clf', lgbm_clf)]),\n",
        "    'XGBoost': Pipeline([('pca', PCA()), ('clf', xgb_clf)]),\n",
        "    'CatBoost': Pipeline([('pca', PCA()), ('clf', cat_clf)]),\n",
        "    'Logistic Regression': Pipeline([('pca', PCA()), ('clf', logreg_clf)]),\n",
        "    'Random Forest': Pipeline([('pca', PCA()), ('clf', rf_clf)]),\n",
        "    'SVM': Pipeline([('pca', PCA()), ('clf', svm_clf)]),\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8AWYHNjkra6"
      },
      "outputs": [],
      "source": [
        "cv_results = []\n",
        "\n",
        "for name, pipeline in pipelines.items():\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall', n_jobs=-1)\n",
        "    cv_results.append((name, scores.mean(), scores.std()))\n",
        "\n",
        "cv_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Classifier performance using cross-validation:\")\n",
        "for name, mean_score, std_dev in cv_results:\n",
        "    print(f\"{name}: {mean_score:.4f} (+/- {std_dev:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Em-9s37kra6"
      },
      "source": [
        "Stacking classifier of PCA analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9GzUbUBkra7"
      },
      "outputs": [],
      "source": [
        "base_classifiers = [\n",
        "    \n",
        "    ('LightGBM', lgbm_clf),\n",
        "    ('XGBoost', xgb_clf),\n",
        "    ('CatBoost', cat_clf),\n",
        "    ('Logistic Regression', logreg_clf),\n",
        "    ('Random Forest', rf_clf),\n",
        "    ('SVM', svm_clf),\n",
        "    \n",
        "]\n",
        "\n",
        "meta_classifier = XGBClassifier(random_state=42)\n",
        "stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le8QrrWMkra7"
      },
      "outputs": [],
      "source": [
        "stacking_scores = cross_val_score(stacking_clf, X_train, y_train, cv=5, scoring='recall', n_jobs=-1)\n",
        "print(\"Stacking Classifier performance:\")\n",
        "print(f\"Mean recall: {stacking_scores.mean():.4f}, Standard deviation: {stacking_scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC7Ri4A1kra7"
      },
      "outputs": [],
      "source": [
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "generate_cm(y_val, stacking_clf.predict(X_val), \"Stacking Classifier\")\n",
        "generate_cm(y_test, stacking_clf.predict(X_test), \"Stacking Classifier\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msXtrEzMkra7"
      },
      "outputs": [],
      "source": [
        "class OptunaWeights:\n",
        "    def __init__(self, random_state):\n",
        "        self.study = None\n",
        "        self.weights = None\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _objective(self, trial, y_true, y_preds):\n",
        "        # Define the weights for the predictions from each model\n",
        "        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n",
        "\n",
        "        # Calculate the weighted prediction\n",
        "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
        "\n",
        "        # Calculate the AUC score for the weighted prediction\n",
        "        score = roc_auc_score(y_true, weighted_pred)\n",
        "        return score\n",
        "\n",
        "    def fit(self, y_true, y_preds, n_trials=2000):\n",
        "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
        "        self.study = optuna.create_study(sampler=sampler, study_name=\"OptunaWeights\", direction='maximize')\n",
        "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
        "        self.study.optimize(objective_partial, n_trials=n_trials)\n",
        "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
        "\n",
        "    def predict(self, y_preds):\n",
        "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
        "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
        "        return weighted_pred\n",
        "\n",
        "    def fit_predict(self, y_true, y_preds, n_trials=2000):\n",
        "        self.fit(y_true, y_preds, n_trials=n_trials)\n",
        "        return self.predict(y_preds)\n",
        "    \n",
        "    def weights(self):\n",
        "        return self.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE-v94sIvY6D"
      },
      "source": [
        "# Train weighted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQRbS89tviVm"
      },
      "outputs": [],
      "source": [
        "kfold = True\n",
        "n_splits = 1 if not kfold else 10\n",
        "random_state = 2023\n",
        "random_state_list = [2140] # used by split_data [71]\n",
        "n_estimators = 9999 # 9999\n",
        "early_stopping_rounds = 200\n",
        "verbose = False\n",
        "device = 'cpu'\n",
        "\n",
        "splitter = Splitter(kfold=kfold, n_splits=n_splits)\n",
        "\n",
        "# Initialize an array for storing test predictions\n",
        "test_predss = np.zeros(X_test.shape[0])\n",
        "ensemble_score = []\n",
        "weights = []\n",
        "trained_models = {'xgb':[], 'lgb':[], 'cat':[]}\n",
        "\n",
        "    \n",
        "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
        "    n = i % n_splits\n",
        "    m = i // n_splits\n",
        "            \n",
        "    # Get a set of Regressor models\n",
        "    classifier = Classifier(n_estimators, device, random_state)\n",
        "    models = classifier.models\n",
        "    \n",
        "    # Initialize lists to store oof and test predictions for each base model\n",
        "    oof_preds = []\n",
        "    test_preds = []\n",
        "    \n",
        "    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
        "    for name, model in models.items():\n",
        "        if name in ['xgb', 'lgb', 'cat']:\n",
        "            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
        "            trained_models[f'{name}'].append(deepcopy(model))\n",
        "        else:\n",
        "            model.fit(X_train_, y_train_)\n",
        "        \n",
        "        test_pred = model.predict_proba(X_test)[:, 1]\n",
        "        y_val_pred = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        score = roc_auc_score(y_val, y_val_pred)\n",
        "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] AUC score: {score:.5f}')\n",
        "        \n",
        "        oof_preds.append(y_val_pred)\n",
        "        test_preds.append(test_pred)\n",
        "    \n",
        "    # Use Optuna to find the best ensemble weights\n",
        "    optweights = OptunaWeights(random_state=random_state)\n",
        "    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
        "    \n",
        "    score = roc_auc_score(y_val, y_val_pred)\n",
        "    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] AUC score {score:.5f}')\n",
        "    ensemble_score.append(score)\n",
        "    weights.append(optweights.weights)\n",
        "    \n",
        "    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
        "    \n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfkEldibvmlO"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean LogLoss score of the ensemble\n",
        "mean_score = np.mean(ensemble_score)\n",
        "std_score = np.std(ensemble_score)\n",
        "print(f'Ensemble Accuracy score {mean_score:.5f} ± {std_score:.5f}')\n",
        "\n",
        "# Print the mean and standard deviation of the ensemble weights for each model\n",
        "print('--- Model Weights ---')\n",
        "mean_weights = np.mean(weights, axis=0)\n",
        "std_weights = np.std(weights, axis=0)\n",
        "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
        "    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}