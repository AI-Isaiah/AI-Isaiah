{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Isaiah/AI-Isaiah/blob/main/20230329_9_ensemble_method_with_smote_optuna%2Bcost_function_AFG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "WuiTAYIhkrav"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "#!pip install optuna\n",
        "#!pip install catboost\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sys\n",
        "sys.path.append('/bin/sample_data')\n",
        "sys.path.append('/root')\n",
        "#from preprocessor import generate_cm\n",
        "from collections import Counter\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTEENN\n",
        "import optuna\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, ParameterSampler, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_auc_score, roc_curve, recall_score, accuracy_score, make_scorer, classification_report\n",
        "from sklearn.feature_selection import RFECV\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import json\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1sDqMMYleLy",
        "outputId": "e0ffc9e3-f4ae-4716-d423-84fff1075433"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  5 06:42:53 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    29W /  70W |    205MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cm(y_true, y_pred, title = \"\"):\n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    labels = ['True', 'False']\n",
        "    sns.set(font_scale=0.8) # adjust font size\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "    # Add labels and title to the plot\n",
        "    plt.ylabel('Predicted Labels')\n",
        "    plt.xlabel('True Labels - updated')\n",
        "    plt.title('Confusion Matrix: ' + title)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Counter\" + title, Counter(y_pred))"
      ],
      "metadata": {
        "id": "gfRrZqwZTCU0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVbI_tqWkrax",
        "outputId": "19efe4bb-9173-47ed-ba26-5d1eb5d6a3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    1\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    1\n",
            "Name: Result, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/sample_data/loan_book_data_labels.csv\")\n",
        "labels = data[\"Result\"]\n",
        "data.drop(\"Result\", axis=1, inplace=True)\n",
        "print(labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_xgboost1(trial, X, y):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1),\n",
        "        'gamma': trial.suggest_float('gamma', 1e-9, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 100),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 100),\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"random_state\": 42,\n",
        "    }\n",
        "            \n",
        "    \n",
        "    # Create an XGBClassifier with the specified hyperparameters\n",
        "    clf = xgb.XGBClassifier(**params, tree_method=\"gpu_hist\", enable_categorical=True, max_cat_to_onehot=1)\n",
        "    clf.fit(X, y, verbose=False)\n",
        "    y_pred = clf.predict(X)\n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "    return accuracy_score(y, y_pred)\n",
        "\n",
        "def objective_low_level(trial, X, y):\n",
        "    \n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1),\n",
        "        'gamma': trial.suggest_float('gamma', 1e-9, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 100),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 100),\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"random_state\": 42,\n",
        "    }\n",
        " \n",
        "    # Create an XGBClassifier with the specified hyperparameters\n",
        "    #clf = xgb.XGBClassifier(**params,enable_categorical=True)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "    #X = X.values   \n",
        "    \n",
        "    dtrain = xgb.DMatrix(X, y, enable_categorical=True)\n",
        "    \n",
        "    bst = xgb.train(params, dtrain, 500, verbose_eval=False, tree_method=\"gpu_hist\", max_cat_to_onehot=1)\n",
        "    #scores = cross_val_score(bst, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "    y_pred = bst.predict(dtrain) #ntree_limit=bst.best_ntree_limit)\n",
        "        \n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "    scores.append(accuracy_score(y, y_pred))\n",
        "    #params['n_estimators'] = num_round\n",
        "\n",
        "\n",
        "    \n",
        "    return np.mean(scores)\n",
        "\n",
        "   \n",
        "\n",
        "def feature_selection2(X, y, max_features=30, n_trials=50):\n",
        "    # Split data for hyperparameter tuning\n",
        "    #X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "     # Create a DMatrix for training\n",
        "    \"\"\"dtrain = xgb.DMatrix(data=X_train, label=y_train, enable_categorical=True)\n",
        "    dtest = xgb.DMatrix(data=X_valid, label=y_valid, enable_categorical=True)\n",
        "    \"\"\"\n",
        "    # Optimize hyperparameters using Optuna\n",
        "    \n",
        "    study = optuna.create_study(direction='maximize', sampler = optuna.samplers.NSGAIISampler(seed = 42)) # update2\n",
        "    study.optimize(lambda trial: objective_xgboost1(trial, X,y), n_trials=n_trials)\n",
        "    best_params = study.best_params\n",
        "    print('Best hyperparameters: {}'.format(best_params))\n",
        "\n",
        "    # Train initial XGBoost model with the best hyperparameters\n",
        "    #num_round = best_params.pop('n_estimators')  # Remove n_estimators from params and store its value\n",
        "    #init_clf = xgb.train(best_params, dtrain, num_round, [(dtest, 'validation')], early_stopping_rounds=100, verbose_eval=False)\n",
        "    init_clf = xgb.XGBClassifier(**best_params, tree_method=\"gpu_hist\", enable_categorical=True, max_cat_to_onehot=1)\n",
        "    init_clf.fit(X,y)\n",
        "\n",
        "    # Get feature importances and indices\n",
        "    \"\"\"importances = init_clf.get_score(importance_type='weight')\n",
        "    indices = sorted(importances, key=lambda x: importances[x], reverse=True)\n",
        "    \"\"\"\n",
        "    importances = init_clf.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Select the top-k features\n",
        "    selected_features = indices[:max_features]\n",
        "    print(\"Indices in function: \", indices)\n",
        "    #X_selected = X[:, selected_features]\n",
        "    \n",
        "\n",
        "    return selected_features\n",
        "\n"
      ],
      "metadata": {
        "id": "Kjm6bGIQx4B-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "OOQGdMWTkray",
        "outputId": "684d1011-32fd-4d96-99d4-001a7451275f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v02_src    category\n",
            "v03_src       int64\n",
            "v04_src    category\n",
            "v05_src    category\n",
            "v06_src    category\n",
            "             ...   \n",
            "v65_src       int64\n",
            "v66_src     float64\n",
            "v67_src     float64\n",
            "v68_src       int64\n",
            "v69_src       int64\n",
            "Length: 68, dtype: object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'label_encoders = {}\\nfor col in cat_columns:\\n    le = LabelEncoder()\\n    X[col] = le.fit_transform(X[col])\\n    label_encoders[col] = le'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "#X = data\n",
        "X = data.drop(\"v01_src\", axis=1)\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = X[col].astype('category')\n",
        "print(X.dtypes)\n",
        "feature_names = X.columns\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "y = y.ravel()\n",
        "cat_columns = X.select_dtypes(include='category').columns\n",
        "\n",
        "num_columns = X.select_dtypes(exclude=['category']).columns\n",
        "\n",
        "# Encode categorical columns using LabelEncoder\n",
        "\"\"\"label_encoders = {}\n",
        "for col in cat_columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8zLBJGsHkraz",
        "outputId": "2098eafb-3c50-4a41-aa17-f71b6c3df0f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.7.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "xgb.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxn2MRMfkraz",
        "outputId": "86336e58-fce6-4b3c-91bd-62f153f2f2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-05 07:57:16,717]\u001b[0m A new study created in memory with name: no-name-fcfa91fe-625f-4a6d-8956-a0d631daa791\u001b[0m\n",
            "\u001b[32m[I 2023-04-05 07:57:17,408]\u001b[0m Trial 0 finished with value: 0.8600973236009732 and parameters: {'n_estimators': 381, 'max_depth': 10, 'learning_rate': 0.7322619478695936, 'gamma': 59.865848420104996, 'subsample': 0.24041677639819287, 'colsample_bytree': 0.2403950683025824, 'reg_alpha': 5.808361217761862, 'reg_lambda': 86.61761457762734, 'min_child_weight': 7}. Best is trial 0 with value: 0.8600973236009732.\u001b[0m\n",
            "\u001b[32m[I 2023-04-05 07:57:18,691]\u001b[0m Trial 1 finished with value: 0.8600973236009732 and parameters: {'n_estimators': 711, 'max_depth': 3, 'learning_rate': 0.9699399423098324, 'gamma': 83.24426408020973, 'subsample': 0.29110519961044856, 'colsample_bytree': 0.26364247048639056, 'reg_alpha': 18.340450986159976, 'reg_lambda': 30.42422429664953, 'min_child_weight': 6}. Best is trial 0 with value: 0.8600973236009732.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'n_estimators': 381, 'max_depth': 10, 'learning_rate': 0.7322619478695936, 'gamma': 59.865848420104996, 'subsample': 0.24041677639819287, 'colsample_bytree': 0.2403950683025824, 'reg_alpha': 5.808361217761862, 'reg_lambda': 86.61761457762734, 'min_child_weight': 7}\n",
            "Indices in function:  [67 24 18 19 20 21 22 23 25 66 26 27 28 29 30 31 17 16 15 14 13 12 11 10\n",
            "  9  8  7  6  5  4  3  2  1 32 33 34 51 65 64 63 62 61 60 59 58 57 56 55\n",
            " 54 53 52 50 35 49 48 47 46 45 44 43 42 41 40 39 38 37 36  0]\n",
            "Cat-cols:  ['v20_src', 'v21_src', 'v22_src', 'v19_src', 'v18_src', 'v13_src', 'v12_src', 'v10_src', 'v07_src', 'v06_src']\n",
            "X_train_shape final:  (1414, 30)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "y_train = pd.DataFrame(y_train, columns=[\"Result\"])\n",
        "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
        "y_val = pd.DataFrame(y_val, columns=[\"Result\"])\n",
        "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
        "y_test = pd.DataFrame(y_test, columns=[\"Result\"])\n",
        "\n",
        "\n",
        "selected_features = feature_selection2(X_train, y_train, 30, 50)\n",
        "\n",
        "\n",
        "\n",
        "cat_columns\n",
        "X_train = X_train.iloc[:, selected_features]\n",
        "X_val = X_val.iloc[:, selected_features]\n",
        "X_test = X_test.iloc[:, selected_features]\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = X_train.select_dtypes(include=['category']).columns.tolist()\n",
        "print(\"Cat-cols: \", cat_cols)\n",
        "num_cols = X_train.select_dtypes(exclude=['category']).columns.tolist()\n",
        "# Create a SMOTENC object with categorical features\n",
        "smote_nc = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in cat_cols], random_state=42)\n",
        "\n",
        "# Fit the SMOTENC object to the training data\n",
        "X_train, y_train = smote_nc.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"X_train_shape final: \", X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "83sm-6FIkra0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit the preprocessor on the training data\n",
        "X_train= preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Apply the preprocessor on the validation and test data\n",
        "X_val = preprocessor.transform(X_val)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUACPgZykra0",
        "outputId": "d1584ace-e750-4ede-897c-d7049b01dbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-05 08:46:43,532]\u001b[0m A new study created in memory with name: no-name-8e4335b4-01c5-47d6-b84c-4eea4c692b16\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return (tn * 4) - (fp * 10)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective_catboost(trial, X, y, cat_cols):\n",
        "    params = {\n",
        "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
        "        'depth': trial.suggest_int('depth', 2, 10),\n",
        "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
        "        'border_count': trial.suggest_int('border_count', 5, 200),\n",
        "        'random_strength': trial.suggest_int('random_strength', 1, 20),\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.01, 100.00),\n",
        "        'sampling_frequency': trial.suggest_categorical('sampling_frequency', [\"PerTree\", \"PerTreeLevel\"]),\n",
        "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n",
        "        'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 1, 10),\n",
        "        'task_type': 'GPU'\n",
        "    }\n",
        "    \n",
        "    n_splits = 5\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Train and evaluate the model using cross-validation\n",
        "    scores = []\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "        \n",
        "        # Create a Pool object with categorical features\n",
        "        pool_train = Pool(X_train, y_train, cat_features=cat_cols)\n",
        "        pool_valid = Pool(X_val, y_val, cat_features=cat_cols)\n",
        "        \n",
        "        # Train the model\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(pool_train, eval_set=pool_valid, verbose=False)\n",
        "        \n",
        "        # Evaluate the model on the validation set\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "        score = custom_loss(y_val, y_pred)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.NSGAIISampler(seed = 42))\n",
        "study.optimize(lambda trial: objective_catboost(trial, X_train,y_train, cat_cols), n_trials=100)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = study.best_params\n",
        "print(\"Best parameters: \", best_params)\n",
        "\n",
        "#Pool variables to be used with catboost:\n",
        "pool_train = Pool(X_train, y_train, cat_features = cat_cols)\n",
        "pool_valid = Pool(X_val, cat_features = cat_cols)\n",
        "pool_test = Pool(X_test, cat_features = cat_cols)\n",
        "\n",
        "\n",
        "# Run the randomized search\n",
        "cb_classifier = CatBoostClassifier(**best_params, random_state=42, verbose=0)\n",
        "cb_classifier.fit(pool_train)\n",
        "tn, fp, fn, tp = confusion_matrix(y_val, cb_classifier.predict(X_val)).ravel()\n",
        "print(\"confusion Matrix validation\")\n",
        "print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp)\n",
        "generate_cm(y_val, cb_classifier.predict(pool_valid), \"CatBoost - Validation\")\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, cb_classifier.predict(X_test)).ravel()\n",
        "print(\"confusion Matrix test\")\n",
        "print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp)\n",
        "generate_cm(y_test, cb_classifier.predict(pool_test), \"CatBoost - Test\")\n",
        "\n",
        "# Save the best parameters for CatBoost\n",
        "with open('cat_best_params.json', 'w') as f:\n",
        "    json.dump(study.best_params, f)\n",
        "\n",
        "# Clear memory\n",
        "del cb_classifier\n",
        "gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEeIXGt0kra1"
      },
      "outputs": [],
      "source": [
        "# Create a RandomForest classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter space for RandomForest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': list(range(50, 500)),\n",
        "    'max_depth': list(range(1, 50)),\n",
        "    'min_samples_split': list(range(2, 50)),\n",
        "    'min_samples_leaf': list(range(1, 50)),\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'bootstrap': [True, False],\n",
        "}\n",
        "\n",
        "# Set up randomized search for RandomForest\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf_clf,\n",
        "    param_distributions=rf_param_grid,\n",
        "    n_iter=100,\n",
        "    scoring=make_scorer(recall_score, greater_is_better=True, pos_label=1),\n",
        "    n_jobs=-1,\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    pre_dispatch=2,\n",
        ")\n",
        "\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "\n",
        "generate_cm(y_val, rf_random_search.predict(X_val))\n",
        "generate_cm(y_test, rf_random_search.predict(X_test))\n",
        "\n",
        "# Save the best parameters for Logistic Regression\n",
        "with open('rf_best_params.json', 'w') as f:\n",
        "    json.dump(rf_random_search.best_params_, f)\n",
        "\n",
        "# Clear memory\n",
        "del rf_random_search\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR5G1utIkra1"
      },
      "outputs": [],
      "source": [
        "# Create a Logistic Regression classifier\n",
        "logreg_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Define the parameter space for Logistic Regression\n",
        "logreg_param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'None'],\n",
        "    'C': list(np.logspace(-4, 4, 1000)),\n",
        "    'fit_intercept': [True, False],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "    'l1_ratio': list(np.linspace(0, 1, 1000)),\n",
        "}\n",
        "\n",
        "# Set up randomized search for Logistic Regression\n",
        "logreg_random_search = RandomizedSearchCV(\n",
        "    estimator=logreg_clf,\n",
        "    param_distributions=logreg_param_grid,\n",
        "    n_iter=100,\n",
        "    scoring=make_scorer(recall_score, greater_is_better=True, pos_label=1),\n",
        "    n_jobs=-1,\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    pre_dispatch=2,\n",
        ")\n",
        "\n",
        "# Run the randomized search\n",
        "logreg_random_search.fit(X_train, y_train)\n",
        "\n",
        "generate_cm(y_val, logreg_random_search.predict(X_val))\n",
        "generate_cm(y_test, logreg_random_search.predict(X_test))\n",
        "\n",
        "# Save the best parameters for Logistic Regression\n",
        "with open('logreg_best_params.json', 'w') as f:\n",
        "    json.dump(logreg_random_search.best_params_, f)\n",
        "\n",
        "# Clear memory\n",
        "del logreg_random_search\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYSwQ_z6kra2"
      },
      "outputs": [],
      "source": [
        "# Create an SVM classifier\n",
        "svm_clf = SVC(random_state=42, probability=True)\n",
        "\n",
        "# Define the parameter space for SVM\n",
        "svm_param_grid = {\n",
        "    'C': list(np.logspace(-4, 4, 1000)),\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'degree': list(range(1, 6)),\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 1, 1000)),\n",
        "    'coef0': list(np.linspace(0, 1, 1000)),\n",
        "    'shrinking': [True, False],\n",
        "}\n",
        "\n",
        "# Set up randomized search for SVM\n",
        "svm_random_search = RandomizedSearchCV(\n",
        "    estimator=svm_clf,\n",
        "    param_distributions=svm_param_grid,\n",
        "    n_iter=100,\n",
        "    scoring=make_scorer(recall_score, greater_is_better=True, pos_label=1),\n",
        "    n_jobs=-1,\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    pre_dispatch=2,\n",
        ")\n",
        "\n",
        "# Run the randomized search\n",
        "svm_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Save the best parameters for SVM\n",
        "with open('svm_best_params.json', 'w') as f:\n",
        "    json.dump(svm_random_search.best_params_, f)\n",
        "\n",
        "generate_cm(y_val, svm_random_search.predict(X_val))\n",
        "generate_cm(y_test, svm_random_search.predict(X_test))\n",
        "\n",
        "# Clear memory\n",
        "del svm_random_search\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA2FOzrEkra2"
      },
      "outputs": [],
      "source": [
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"X_val shape: \", X_val.shape)\n",
        "print(\"X_test shape: \", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP3kkZe3kra2"
      },
      "outputs": [],
      "source": [
        "lgbm_clf = LGBMClassifier(random_state=42)\n",
        "lgbm_params = {\n",
        "    'learning_rate': np.logspace(-3, 0, 10),\n",
        "    'n_estimators': range(50, 500),\n",
        "    'num_leaves': range(20, 100),\n",
        "    'min_child_samples': range(5, 50),\n",
        "    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
        "    'subsample': np.linspace(0.1, 1, 10),\n",
        "    'colsample_bytree': np.linspace(0.1, 1, 10),\n",
        "    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
        "}\n",
        "\n",
        "recall_scorer = make_scorer(recall_score, greater_is_better=True)\n",
        "\n",
        "lgbm_random_search = RandomizedSearchCV(lgbm_clf, param_distributions=lgbm_params, n_iter=50, scoring=recall_scorer, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
        "lgbm_random_search.fit(X_train, y_train)\n",
        "\n",
        "lgbm_best_params = lgbm_random_search.best_params_\n",
        "print(lgbm_best_params)\n",
        "\n",
        "generate_cm(y_val, lgbm_random_search.predict(X_val))\n",
        "generate_cm(y_test, lgbm_random_search.predict(X_test))\n",
        "\n",
        "with open('lgbm_best_params.json', 'w') as f:\n",
        "    json.dump(lgbm_best_params, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEn-9YwUkra3"
      },
      "outputs": [],
      "source": [
        "xgb_clf = XGBClassifier(use_label_encoder=False, random_state=42, tree_method='hist', n_jobs=-1)\n",
        "xgb_params = {\n",
        "    'learning_rate': np.logspace(-3, 0, 10),\n",
        "    'n_estimators': range(50, 500),\n",
        "    'max_depth': range(3, 15),\n",
        "    'min_child_weight': range(1, 10),\n",
        "    'gamma': np.linspace(0, 1, 11),\n",
        "    'subsample': np.linspace(0.1, 1, 10),\n",
        "    'colsample_bytree': np.linspace(0.1, 1, 10),\n",
        "    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
        "}\n",
        "\n",
        "xgb_random_search = RandomizedSearchCV(xgb_clf, param_distributions=xgb_params, n_iter=50, scoring=recall_scorer, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "\n",
        "xgb_best_params = xgb_random_search.best_params_\n",
        "print(xgb_best_params)\n",
        "\n",
        "with open('xgb_best_params.json', 'w') as f:\n",
        "    json.dump(xgb_best_params, f)\n",
        "\n",
        "generate_cm(y_val, xgb_random_search.predict(X_val), \"XGBoost, Randomized Search, Validation Set\")\n",
        "generate_cm(y_test, xgb_random_search.predict(X_test), \"XGBoost, Randomized Search, Validation Set\")\n",
        "\n",
        "print(\"Counter - valid\" , Counter(y_val))\n",
        "print(\"Counter - test\", Counter(y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frFZ_RdRkra3"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Load the best parameters for each classifier\n",
        "with open('lgbm_best_params.json', 'r') as f:\n",
        "    lgbm_best_params = json.load(f)\n",
        "\n",
        "with open('xgb_best_params.json', 'r') as f:\n",
        "    xgb_best_params = json.load(f)\n",
        "\n",
        "with open('cat_best_params.json', 'r') as f:\n",
        "    cat_best_params = json.load(f)\n",
        "\n",
        "with open('logreg_best_params.json', 'r') as f:\n",
        "    logreg_best_params = json.load(f)\n",
        "\n",
        "with open('rf_best_params.json', 'r') as f:\n",
        "    rf_best_params = json.load(f)\n",
        "\n",
        "with open('svm_best_params.json', 'r') as f:\n",
        "    svm_best_params = json.load(f)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jafTzVDLkra3"
      },
      "outputs": [],
      "source": [
        "# Instantiate classifiers with the best parameters\n",
        "lgbm_clf = LGBMClassifier(random_state=42, **lgbm_best_params)\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, random_state=42, tree_method='hist', n_jobs=-1, **xgb_best_params)\n",
        "cat_clf = CatBoostClassifier(task_type=\"CPU\", random_seed=42, verbose = False, **cat_best_params)\n",
        "logreg_clf = LogisticRegression(random_state=42, **logreg_best_params)\n",
        "rf_clf = RandomForestClassifier(random_state=42, **rf_best_params)\n",
        "svm_clf = SVC(probability=True, random_state=42, **svm_best_params)\n",
        "\n",
        "# Create a soft-voting classifier\n",
        "ensemble_clf = VotingClassifier(estimators=[\n",
        "    ('lgbm', lgbm_clf),\n",
        "    ('xgb', xgb_clf),\n",
        "    ('cat', cat_clf),\n",
        "    ('logreg', logreg_clf),\n",
        "    ('rf', rf_clf),\n",
        "    ('svm', svm_clf)\n",
        "], voting='soft')\n",
        "\n",
        "\n",
        "\n",
        "# Train the ensemble classifier\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the recall score for the ensemble\n",
        "ensemble_recall = recall_score(y_true=y_test, y_pred=ensemble_clf.predict(X_test), pos_label=1)\n",
        "print(f\"Ensemble recall score: {ensemble_recall}\")\n",
        "\n",
        "generate_cm(y_val, ensemble_clf.predict(X_val))\n",
        "generate_cm(y_test, ensemble_clf.predict(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjmuYe8qkra3"
      },
      "outputs": [],
      "source": [
        "y_pred = ensemble_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSIzz7Wnkra4"
      },
      "outputs": [],
      "source": [
        "y_pred = ensemble_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDcehakvkra4"
      },
      "outputs": [],
      "source": [
        "precision = precision_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
        "recall = recall_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
        "f1 = f1_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCpwFDOakra4"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = ensemble_clf.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_true=y_test, y_score=y_pred_proba)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true=y_test, y_score=y_pred_proba)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"Ensemble Classifier (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkwv32W-kra4"
      },
      "source": [
        "Addition of feauture reduction methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG03O_2Gkra5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "def voting_classifier_feature_importances(voting_clf):\n",
        "    importances = []\n",
        "    for clf in voting_clf.estimators_:\n",
        "        if isinstance(clf, tuple):  # Check if the clf is a tuple (name, estimator)\n",
        "            clf = clf[1]\n",
        "        if hasattr(clf, 'coef_'):\n",
        "            importances.append(clf.coef_.flatten())\n",
        "        elif hasattr(clf, 'feature_importances_'):\n",
        "            importances.append(clf.feature_importances_)\n",
        "    \n",
        "    if not importances:\n",
        "        raise ValueError(\"No estimators in the ensemble have `coef_` or `feature_importances_` attribute.\")\n",
        "    \n",
        "    return np.mean(importances, axis=0)\n",
        "\n",
        "\n",
        "rfe = RFECV(estimator=ensemble_clf, step=1, cv=5, scoring='recall', n_jobs=-1, importance_getter=voting_classifier_feature_importances)\n",
        "\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    with redirect_stdout(devnull):\n",
        "        rfe.fit(X_train, y_train)\n",
        "\n",
        "# Print the optimal number of features\n",
        "print(\"Optimal number of features:\", rfe.n_features_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cevSlb2zkra5"
      },
      "outputs": [],
      "source": [
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "# Print the optimal number of features\n",
        "print(\"Optimal number of features:\", rfe.n_features_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJdTPStqkra5"
      },
      "outputs": [],
      "source": [
        "ensemble_clf.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = ensemble_clf.predict(X_test_rfe)\n",
        "\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "report_rfe = classification_report(y_test, y_pred_rfe)\n",
        "conf_matrix_rfe = confusion_matrix(y_test, y_pred_rfe)\n",
        "\n",
        "print(f\"Accuracy (with RFE): {accuracy_rfe:.4f}\")\n",
        "print(\"Classification Report (with RFE):\")\n",
        "print(report_rfe)\n",
        "print(\"Confusion Matrix (with RFE):\")\n",
        "print(conf_matrix_rfe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodr_9e9kra6"
      },
      "source": [
        "Use stacking method to combine the results of different models\n",
        "Extra trees classifier and Principal component analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQWHxfm9kra6"
      },
      "outputs": [],
      "source": [
        "ex_clf = ExtraTreesClassifier(random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pca', PCA()),\n",
        "    ('ex_clf', ex_clf),\n",
        "])\n",
        "param_grid = {\n",
        "    'pca__n_components': np.arange(1, X_train.shape[1] + 1),\n",
        "    'ex_clf__n_estimators': [100, 200, 500],\n",
        "    'ex_clf__max_depth': [None, 10, 20, 30],\n",
        "    'ex_clf__min_samples_split': [2, 5, 10],\n",
        "    'ex_clf__min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(pipeline, param_grid, n_iter=50, cv=5, scoring='recall', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        "print(\"Best score:\", random_search.best_score_)\n",
        "ex_clf_best_params = random_search.best_params_\n",
        "\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6DTlRckra6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Mrf_qNkra6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(X_train.shape[1]), importances[sorted_indices])\n",
        "plt.xticks(range(X_train.shape[1]), sorted_indices)\n",
        "plt.xlabel(\"Feature index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Feature importances in the best ExtraTreesClassifier model\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOa55XBekra6"
      },
      "source": [
        "Use the other classifiers to predict the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6SAIsSVkra6"
      },
      "outputs": [],
      "source": [
        "# Instantiate classifiers\n",
        "\n",
        "lgbm_clf = LGBMClassifier(random_state=42, **lgbm_best_params)\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, random_state=42, tree_method='hist', n_jobs=-1, **xgb_best_params)\n",
        "cat_clf = CatBoostClassifier(task_type=\"CPU\", random_seed=42, **cat_best_params)\n",
        "logreg_clf = LogisticRegression(random_state=42, **logreg_best_params)\n",
        "rf_clf = RandomForestClassifier(random_state=42, **rf_best_params)\n",
        "svm_clf = SVC(probability=True, random_state=42, **svm_best_params)\n",
        "\n",
        "# Create pipelines with PCA and classifiers\n",
        "pipelines = {\n",
        "    \n",
        "    'LightGBM': Pipeline([('pca', PCA()), ('clf', lgbm_clf)]),\n",
        "    'XGBoost': Pipeline([('pca', PCA()), ('clf', xgb_clf)]),\n",
        "    'CatBoost': Pipeline([('pca', PCA()), ('clf', cat_clf)]),\n",
        "    'Logistic Regression': Pipeline([('pca', PCA()), ('clf', logreg_clf)]),\n",
        "    'Random Forest': Pipeline([('pca', PCA()), ('clf', rf_clf)]),\n",
        "    'SVM': Pipeline([('pca', PCA()), ('clf', svm_clf)]),\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8AWYHNjkra6"
      },
      "outputs": [],
      "source": [
        "cv_results = []\n",
        "\n",
        "for name, pipeline in pipelines.items():\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall', n_jobs=-1)\n",
        "    cv_results.append((name, scores.mean(), scores.std()))\n",
        "\n",
        "cv_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Classifier performance using cross-validation:\")\n",
        "for name, mean_score, std_dev in cv_results:\n",
        "    print(f\"{name}: {mean_score:.4f} (+/- {std_dev:.4f})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Em-9s37kra6"
      },
      "source": [
        "Stacking classifier of PCA analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9GzUbUBkra7"
      },
      "outputs": [],
      "source": [
        "base_classifiers = [\n",
        "    \n",
        "    ('LightGBM', lgbm_clf),\n",
        "    ('XGBoost', xgb_clf),\n",
        "    ('CatBoost', cat_clf),\n",
        "    ('Logistic Regression', logreg_clf),\n",
        "    ('Random Forest', rf_clf),\n",
        "    ('SVM', svm_clf),\n",
        "    \n",
        "]\n",
        "\n",
        "meta_classifier = XGBClassifier(random_state=42)\n",
        "stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le8QrrWMkra7"
      },
      "outputs": [],
      "source": [
        "stacking_scores = cross_val_score(stacking_clf, X_train, y_train, cv=5, scoring='recall', n_jobs=-1)\n",
        "print(\"Stacking Classifier performance:\")\n",
        "print(f\"Mean recall: {stacking_scores.mean():.4f}, Standard deviation: {stacking_scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC7Ri4A1kra7"
      },
      "outputs": [],
      "source": [
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "generate_cm(y_val, stacking_clf.predict(X_val), \"Stacking Classifier\")\n",
        "generate_cm(y_test, stacking_clf.predict(X_test), \"Stacking Classifier\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msXtrEzMkra7"
      },
      "outputs": [],
      "source": [
        "#count the values of y array\n",
        "from collections import Counter\n",
        "print(Counter(y_test))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}